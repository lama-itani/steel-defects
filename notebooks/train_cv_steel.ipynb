{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7b55a8",
   "metadata": {},
   "source": [
    "# 1/ Env Setup\n",
    "Load necessary libraries to run this notebook. <br>\n",
    "All libraries are cited in ```requirements.txt```. <br>\n",
    "Documentation: https://docs.pytorch.org/vision/main/models/generated/torchvision.models.detection.retinanet_resnet50_fpn_v2.html\n",
    "\n",
    "## 1.1/ Import dependencies\n",
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a2cafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: /Users/litani/Documents/myCode/steel-defects\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd() # path to the current working directory (notebook location)\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\")) # path to project root\n",
    "\n",
    "if project_root not in sys.path: # add project root to sys.path\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"Project root added to sys.path: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce85e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch \n",
    "from torchvision.models.detection import retinanet_resnet50_fpn_v2\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c148bd",
   "metadata": {},
   "source": [
    "## 1.2/ Set reproducibility\n",
    "Device and seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6e9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1f95b",
   "metadata": {},
   "source": [
    "# 2/ Configuration Management\n",
    "Define:\n",
    "- image path\n",
    "- model hyperparameters\n",
    "- hardware\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda84995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATA_ROOT = Path(project_root) / \"data\" / \"raw\"\n",
    "    TRAIN_IMG = DATA_ROOT / \"train_images\"\n",
    "    TRAIN_ANN = DATA_ROOT / \"train_annotations\"\n",
    "    VAL_IMG = DATA_ROOT / \"valid_images\"\n",
    "    VAL_ANN = DATA_ROOT / \"valid_annotations\"\n",
    "\n",
    "    # Model parameters\n",
    "    NUM_CLASSES = 7 # 6 defects + 1 background\n",
    "    BACKBONE_PRETRAINED = True \n",
    "    \n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 5  # no mention of batch size in the paper (go for > 10 when you are sure that training works)\n",
    "    NUM_EPOCHS = 3 # 24 epochs based on paper. Reduced for quicker testing\n",
    "    LEARNING_RATE = 0.0025 # 0.0025 based on paper\n",
    "    MOMENTUM = 0.9 # 0.9 based on paper\n",
    "    WEIGHT_DECAY = 0.0005 # double check this value <<<<<<<\n",
    "\n",
    "    # Hardware\n",
    "    DEVICE = device\n",
    "    NUM_WORKERS = 8\n",
    "    PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0cc310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Config at 0x12de812b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33c203",
   "metadata": {},
   "source": [
    "# 3/ Dataset Class\n",
    "- Load images and annotations into PyTorch format. \n",
    "- This is necessary since RetineNet excepts a dictionary format. The latter requires XML parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb61e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.dataset import SteelDefectDataset, collate_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038647b",
   "metadata": {},
   "source": [
    "# 4/ Data Augmentation\n",
    "- We have 1800 images, resorting to image augmentation is mandatory to avoid overfitting. \n",
    "- Geometric transformation, simplist form, will be applied as a quick fix:\n",
    "    - Horizental/Vertical flips\n",
    "    - Rotate by 90\n",
    "    - Others: brightness, contrast, adding random noise\n",
    "- **NB:** OpenCV stores images as [Height in pixels, Width in pixels, RGB] while PyTorch expects [channel, height, width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62e0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.transforms_pipeline import get_train_transforms, get_val_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aec8b1",
   "metadata": {},
   "source": [
    "# 5/ Model Initilization\n",
    "- Apply transfer learning where pretrained RetineNet is loaded the changes are applied based on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60474ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes, pretrained = True):\n",
    "    # Load pretrained RetineNet w/ ResNet50 backbone,\n",
    "    model = retinanet_resnet50_fpn_v2(weights = \"DEFAULT\" if pretrained else None)  # DEFAULT loads ImageNet pretrained weights for transfer learning\n",
    "    \n",
    "    # Replace head so that model learns defect-specific patterns\n",
    "    num_anchors = model.head.classification_head.num_anchors # default is 9 anchors per location >> 3 scales x 3 aspect ratios\n",
    "    model.head.classification_head = RetinaNetClassificationHead(\n",
    "        in_channels = 256,          # Input: 256 features from FPN   \n",
    "        num_anchors = num_anchors,  # Process: 9 anchors per location\n",
    "        num_classes = num_classes   # Output: 7 classes scores per anchor\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model(config.NUM_CLASSES).to(device) # Create model instance and move to device CPU/GPU, config.NUM_CLASSES = 7 includes background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974a65a",
   "metadata": {},
   "source": [
    "# 6/ Data Loaders\n",
    "- collate_func is a fucntion that works on the collation process of RetinaNet since images have variable bbox counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b547d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/litani/Documents/myCode/steel-defects/venv-CVsteel/lib/python3.14/site-packages/albumentations/core/composition.py:359: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    }
   ],
   "source": [
    "# Create train dataset with augmentations and val dataset without augmentations, only format conversion\n",
    "train_dataset = SteelDefectDataset(\n",
    "    config.TRAIN_IMG,\n",
    "    config.TRAIN_ANN,\n",
    "    transforms = get_train_transforms()\n",
    ")\n",
    "\n",
    "val_dataset = SteelDefectDataset(\n",
    "    config.VAL_IMG,\n",
    "    config.VAL_ANN,\n",
    "    transforms = get_val_transforms()\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = config.BATCH_SIZE,\n",
    "    shuffle = True, # shuffle training data for better generalization\n",
    "    num_workers = config.NUM_WORKERS,\n",
    "    pin_memory = config.PIN_MEMORY, # only useful if using GPU\n",
    "    collate_fn = collate_func \n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = config.BATCH_SIZE,\n",
    "    shuffle = False, # validation data should be consistent\n",
    "    num_workers = config.NUM_WORKERS,\n",
    "    pin_memory = config.PIN_MEMORY,\n",
    "    collate_fn = collate_func\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf48c8",
   "metadata": {},
   "source": [
    "# 7/ Training Loop and Evaluation Metrics\n",
    "- Quick and dirty: use SGD as an optimizer for an initial model training, won't be launching/tracking experiments in the beginning\n",
    "- this is standard supervised learning using RetinaNet loss\n",
    "- For evaluation, assess model perf without retraining, aim for .5 (50% overlap) with validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8d5508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.trainEval_pipeline import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ba6f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Epoch 1/3\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3562\n",
      "Val mAP@0.5: 0.0000\n",
      "Learning Rate: 0.002500\n",
      "✓ Checkpoint saved: models/retinanet_epoch_1.pth\n",
      "\n",
      "============================================================\n",
      "Epoch 2/3\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/litani/Documents/myCode/steel-defects/venv-CVsteel/lib/python3.14/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Encountered more than 100 detections in a single image. This means that certain detections with the lowest scores will be ignored, that may have an undesirable impact on performance. Please consider adjusting the `max_detection_threshold` to suit your use case. To disable this warning, set attribute class `warn_on_many_detections=False`, after initializing the metric.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0409\n",
      "Val mAP@0.5: 0.2362\n",
      "Learning Rate: 0.002500\n",
      "✓ Checkpoint saved: models/retinanet_epoch_2.pth\n",
      "\n",
      "============================================================\n",
      "Epoch 3/3\n",
      "============================================================\n",
      "Train Loss: 0.8673\n",
      "Val mAP@0.5: 0.3986\n",
      "Learning Rate: 0.002500\n",
      "✓ Checkpoint saved: models/retinanet_epoch_3.pth\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr = config.LEARNING_RATE,\n",
    "    momentum = config.MOMENTUM,\n",
    "    weight_decay = config.WEIGHT_DECAY # >>> double check this value <<<\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size = 5, \n",
    "    gamma = 0.8   # reduce to 80% every 5 epochs\n",
    ")\n",
    "\n",
    "# Checkpoint directory\n",
    "Path(\"models\").mkdir(exist_ok = True)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch + 1}/{config.NUM_EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation\n",
    "    results = evaluate(model, val_loader, device)\n",
    "    \n",
    "    # Display metrics\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val mAP@0.5: {results['map_50']:.4f}\")\n",
    "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    save_interval = max(1, config.NUM_EPOCHS // 3)\n",
    "    if (epoch + 1) % save_interval == 0 or (epoch + 1) == config.NUM_EPOCHS:\n",
    "        checkpoint_path = f\"models/retinanet_epoch_{epoch+1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"✓ Checkpoint saved: {checkpoint_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-CVsteel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
