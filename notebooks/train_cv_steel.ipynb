{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7b55a8",
   "metadata": {},
   "source": [
    "# 1/ Env Setup\n",
    "Load necessary libraries to run this notebook. <br>\n",
    "All libraries are cited in ```requirements.txt```. <br>\n",
    "Documentation: https://docs.pytorch.org/vision/main/models/generated/torchvision.models.detection.retinanet_resnet50_fpn_v2.html\n",
    "\n",
    "## 1.1/ Import dependencies\n",
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99a2cafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: /Users/litani/Documents/myCode/steel-defects\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd() # path to the current working directory (notebook location)\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"..\")) # path to project root\n",
    "\n",
    "if project_root not in sys.path: # add project root to sys.path\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"Project root added to sys.path: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce85e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch \n",
    "from torchvision.models.detection import retinanet_resnet50_fpn_v2\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c148bd",
   "metadata": {},
   "source": [
    "## 1.2/ Set reproducibility\n",
    "Device and seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf6e9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1f95b",
   "metadata": {},
   "source": [
    "# 2/ Configuration Management\n",
    "Define:\n",
    "- image path\n",
    "- model hyperparameters\n",
    "- hardware\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda84995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATA_ROOT = Path(project_root) / \"data\" / \"raw\"\n",
    "    TRAIN_IMG = DATA_ROOT / \"train_images\"\n",
    "    TRAIN_ANN = DATA_ROOT / \"train_annotations\"\n",
    "    VAL_IMG = DATA_ROOT / \"valid_images\"\n",
    "    VAL_ANN = DATA_ROOT / \"valid_annotations\"\n",
    "\n",
    "    # Model parameters\n",
    "    NUM_CLASSES = 7 # 6 defects + 1 background\n",
    "    BACKBONE_PRETRAINED = True \n",
    "    \n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 16  # no mention of batch size in the paper (go for > 10 when you are sure that training works)\n",
    "    NUM_EPOCHS = 24 # 24 epochs based on paper. Reduced for quicker testing\n",
    "    LEARNING_RATE = .002 # 0.0025 based on paper\n",
    "    MOMENTUM = .9 # 0.9 based on paper\n",
    "    WEIGHT_DECAY = .0005 # double check this value <<<<<<<\n",
    "\n",
    "    # Hardware\n",
    "    DEVICE = device\n",
    "    NUM_WORKERS = 8\n",
    "    PIN_MEMORY = False # True if torch.cuda.is_available() else False [in case of GPU avaliable]\n",
    "\n",
    "    # Visualization\n",
    "    SAVE_PLOTS = True\n",
    "    PLOT_INTERVAL = 4\n",
    "\n",
    "    # Scheduler parameters\n",
    "    STEP_SIZE = 8 # based on paper\n",
    "    GAMMA = .5 \n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d0cc310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Config at 0x10e9b7cb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33c203",
   "metadata": {},
   "source": [
    "# 3/ Dataset Class\n",
    "- Load images and annotations into PyTorch format. \n",
    "- This is necessary since RetineNet excepts a dictionary format. The latter requires XML parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2eb61e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.dataset import SteelDefectDataset, collate_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038647b",
   "metadata": {},
   "source": [
    "# 4/ Data Augmentation\n",
    "- We have 1800 images, resorting to image augmentation is mandatory to avoid overfitting. \n",
    "- Geometric transformation, simplist form, will be applied as a quick fix:\n",
    "    - Horizental/Vertical flips\n",
    "    - Rotate by 90\n",
    "    - Others: brightness, contrast, adding random noise\n",
    "- **NB:** OpenCV stores images as [Height in pixels, Width in pixels, RGB] while PyTorch expects [channel, height, width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d62e0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.transforms_pipeline import get_train_transforms, get_val_transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aec8b1",
   "metadata": {},
   "source": [
    "# 5/ Model Initilization\n",
    "- Apply transfer learning where pretrained RetineNet is loaded the changes are applied based on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e60474ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes, pretrained = True):\n",
    "    # Load pretrained RetinaNet w/ ResNet50 backbone,\n",
    "    model = retinanet_resnet50_fpn_v2(weights = \"DEFAULT\" if pretrained else None)  # DEFAULT loads ImageNet pretrained weights for transfer learning\n",
    "    \n",
    "    # Replace head so that model learns defect-specific patterns\n",
    "    num_anchors = model.head.classification_head.num_anchors # default is 9 anchors per location >> 3 scales x 3 aspect ratios\n",
    "    model.head.classification_head = RetinaNetClassificationHead(\n",
    "        in_channels = 256,          # Input: 256 features from FPN   \n",
    "        num_anchors = num_anchors,  # Process: 9 anchors per location\n",
    "        num_classes = num_classes   # Output: 7 classes scores per anchor\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model(config.NUM_CLASSES).to(device) # Create model instance and move to device CPU/GPU, config.NUM_CLASSES = 7 includes background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974a65a",
   "metadata": {},
   "source": [
    "# 6/ Data Loaders\n",
    "- collate_func is a fucntion that works on the collation process of RetinaNet since images have variable bbox counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50b547d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/litani/Documents/myCode/steel-defects/venv-CVsteel/lib/python3.14/site-packages/albumentations/core/composition.py:359: UserWarning: Got processor for bboxes, but no transform to process it.\n",
      "  self._set_keys()\n"
     ]
    }
   ],
   "source": [
    "# Create train dataset with augmentations and val dataset without augmentations, only format conversion\n",
    "train_dataset = SteelDefectDataset(\n",
    "    config.TRAIN_IMG,\n",
    "    config.TRAIN_ANN,\n",
    "    transforms = get_train_transforms()\n",
    ")\n",
    "\n",
    "val_dataset = SteelDefectDataset(\n",
    "    config.VAL_IMG,\n",
    "    config.VAL_ANN,\n",
    "    transforms = get_val_transforms()\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = config.BATCH_SIZE,\n",
    "    shuffle = True, # shuffle training data for better generalization\n",
    "    num_workers = config.NUM_WORKERS,\n",
    "    pin_memory = config.PIN_MEMORY, # only useful if using GPU\n",
    "    collate_fn = collate_func \n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = config.BATCH_SIZE,\n",
    "    shuffle = False, # validation data should be consistent\n",
    "    num_workers = config.NUM_WORKERS,\n",
    "    pin_memory = config.PIN_MEMORY,\n",
    "    collate_fn = collate_func\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f7911",
   "metadata": {},
   "source": [
    "# 7/ Training Set-up and Visualization Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afa394cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.trainEval_pipeline import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6522bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history storage\n",
    "history = defaultdict(list)\n",
    "\n",
    "def plot_training_curves(history, save_path = 'training_curves.png'):\n",
    "    \"\"\"\n",
    "    Create publication-ready training curves.\n",
    "    \n",
    "    Visualizations:\n",
    "    1. Loss curves (train/val) - Shows optimization progress\n",
    "    2. mAP@0.5 - Primary detection metric\n",
    "    3. Learning rate schedule - Shows adaptation strategy\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize = (14, 10))\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # 1. Training Loss\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-o', label = 'Train Loss', linewidth = 2, markersize = 4)\n",
    "    axes[0, 0].set_xlabel('Epoch', fontsize = 11)\n",
    "    axes[0, 0].set_ylabel('Loss', fontsize = 11)\n",
    "    axes[0, 0].set_title('Training Loss Convergence', fontsize = 12, fontweight = 'bold')\n",
    "    axes[0, 0].grid(True, alpha = 0.3)\n",
    "    axes[0, 0].legend(fontsize = 10)\n",
    "    \n",
    "    # 2. Validation mAP@0.5\n",
    "    axes[0, 1].plot(epochs, history['val_map50'], 'g-s', label = 'Val mAP@0.5', linewidth = 2, markersize = 4)\n",
    "    axes[0, 1].axhline(y = 0.5, color = 'r', linestyle = '--', alpha = 0.5, label = 'Target (0.5)')\n",
    "    axes[0, 1].set_xlabel('Epoch', fontsize = 11)\n",
    "    axes[0, 1].set_ylabel('mAP@0.5', fontsize = 11)\n",
    "    axes[0, 1].set_title('Validation mAP@0.5 Progress', fontsize = 12, fontweight = 'bold')\n",
    "    axes[0, 1].grid(True, alpha = 0.3)\n",
    "    axes[0, 1].legend(fontsize = 10)\n",
    "    axes[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # 3. Learning Rate Schedule\n",
    "    axes[1, 0].plot(epochs, history['learning_rate'], 'r-^', linewidth = 2, markersize = 4)\n",
    "    axes[1, 0].set_xlabel('Epoch', fontsize = 11)\n",
    "    axes[1, 0].set_ylabel('Learning Rate', fontsize = 11)\n",
    "    axes[1, 0].set_title('Learning Rate Schedule', fontsize = 12, fontweight = 'bold')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True, alpha = 0.3)\n",
    "    \n",
    "    # 4. Combined Loss & mAP (dual axis)\n",
    "    ax1 = axes[1, 1]\n",
    "    ax2 = ax1.twinx()\n",
    "    \n",
    "    line1 = ax1.plot(epochs, history['train_loss'], 'b-o', label = 'Train Loss', linewidth = 2, markersize = 4)\n",
    "    line2 = ax2.plot(epochs, history['val_map50'], 'g-s', label = 'Val mAP@0.5', linewidth = 2, markersize = 4)\n",
    "    \n",
    "    ax1.set_xlabel('Epoch', fontsize = 11)\n",
    "    ax1.set_ylabel('Loss', fontsize = 11, color = 'b')\n",
    "    ax2.set_ylabel('mAP@0.5', fontsize = 11, color = 'g')\n",
    "    ax1.set_title('Loss vs mAP@0.5', fontsize = 12, fontweight = 'bold')\n",
    "    ax1.tick_params(axis = 'y', labelcolor = 'b')\n",
    "    ax2.tick_params(axis = 'y', labelcolor = 'g')\n",
    "    ax1.grid(True, alpha = 0.3)\n",
    "    \n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, loc = 'center right', fontsize = 10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi = 300, bbox_inches = 'tight')\n",
    "    plt.close()\n",
    "    print(f\"Training curves saved: {save_path}\")\n",
    "\n",
    "def save_training_metrics(history, save_path = 'training_metrics.json'):\n",
    "    \"\"\"Export metrics for later analysis/reporting.\"\"\"\n",
    "    clean_history = {\n",
    "        k: [float(v) if torch.is_tensor(v) else v for v in vals]\n",
    "        for k, vals in history.items()\n",
    "    }\n",
    "    with open(save_path, 'w') as f:\n",
    "        json.dump(dict(history), f, indent = 2)\n",
    "    print(f\"Metrics saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cf48c8",
   "metadata": {},
   "source": [
    "# 8/ Training Loop and Evaluation Metrics\n",
    "- Quick and dirty: use SGD as an optimizer for an initial model training, won't be launching/tracking experiments in the beginning\n",
    "- this is standard supervised learning using RetinaNet loss\n",
    "- For evaluation, assess model perf without retraining, aim for .5 (50% overlap) with validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8d5508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.trainEval_pipeline import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7ba6f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Starting training: 24 epochs\n",
      "Batch size: 16 | LR: 0.002\n",
      "Device: mps | Workers: 8\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Epoch 1/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 1.5642\n",
      "  Batch [20/107] Loss: 1.5910\n",
      "  Batch [30/107] Loss: 1.5537\n",
      "  Batch [40/107] Loss: 1.5244\n",
      "  Batch [50/107] Loss: 1.5331\n",
      "  Batch [60/107] Loss: 1.5259\n",
      "  Batch [70/107] Loss: 1.5183\n",
      "  Batch [80/107] Loss: 1.5455\n",
      "  Batch [90/107] Loss: 1.4931\n",
      "  Batch [100/107] Loss: 1.4929\n",
      "Train Loss: 1.5289\n",
      "Val mAP@0.5: 0.0000\n",
      "Learning Rate: 0.002000\n",
      "\n",
      "======================================================================\n",
      "Epoch 2/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 1.4685\n",
      "  Batch [20/107] Loss: 1.4716\n",
      "  Batch [30/107] Loss: 1.4563\n",
      "  Batch [40/107] Loss: 1.4162\n",
      "  Batch [50/107] Loss: 1.4002\n",
      "  Batch [60/107] Loss: 1.4237\n",
      "  Batch [70/107] Loss: 1.2105\n",
      "  Batch [80/107] Loss: 1.1789\n",
      "  Batch [90/107] Loss: 1.1748\n",
      "  Batch [100/107] Loss: 1.0784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/litani/Documents/myCode/steel-defects/venv-CVsteel/lib/python3.14/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Encountered more than 100 detections in a single image. This means that certain detections with the lowest scores will be ignored, that may have an undesirable impact on performance. Please consider adjusting the `max_detection_threshold` to suit your use case. To disable this warning, set attribute class `warn_on_many_detections=False`, after initializing the metric.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3278\n",
      "Val mAP@0.5: 0.0778\n",
      "Learning Rate: 0.002000\n",
      "✓ Best model saved (mAP: 0.0778)\n",
      "\n",
      "======================================================================\n",
      "Epoch 3/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.9305\n",
      "  Batch [20/107] Loss: 1.0087\n",
      "  Batch [30/107] Loss: 1.0501\n",
      "  Batch [40/107] Loss: 1.0462\n",
      "  Batch [50/107] Loss: 0.9629\n",
      "  Batch [60/107] Loss: 0.9672\n",
      "  Batch [70/107] Loss: 0.9338\n",
      "  Batch [80/107] Loss: 1.0362\n",
      "  Batch [90/107] Loss: 0.9551\n",
      "  Batch [100/107] Loss: 0.8885\n",
      "Train Loss: 0.9977\n",
      "Val mAP@0.5: 0.1778\n",
      "Learning Rate: 0.002000\n",
      "✓ Best model saved (mAP: 0.1778)\n",
      "\n",
      "======================================================================\n",
      "Epoch 4/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.9937\n",
      "  Batch [20/107] Loss: 0.9958\n",
      "  Batch [30/107] Loss: 0.8558\n",
      "  Batch [40/107] Loss: 0.7590\n",
      "  Batch [50/107] Loss: 0.9295\n",
      "  Batch [60/107] Loss: 0.9533\n",
      "  Batch [70/107] Loss: 0.7299\n",
      "  Batch [80/107] Loss: 0.7775\n",
      "  Batch [90/107] Loss: 0.8031\n",
      "  Batch [100/107] Loss: 0.9154\n",
      "Train Loss: 0.8956\n",
      "Val mAP@0.5: 0.2999\n",
      "Learning Rate: 0.002000\n",
      "✓ Best model saved (mAP: 0.2999)\n",
      "Training curves saved: visualizations/training_curves_epoch4.png\n",
      "\n",
      "======================================================================\n",
      "Epoch 5/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.6997\n",
      "  Batch [20/107] Loss: 1.0326\n",
      "  Batch [30/107] Loss: 0.6940\n",
      "  Batch [40/107] Loss: 0.9066\n",
      "  Batch [50/107] Loss: 0.7073\n",
      "  Batch [60/107] Loss: 0.8176\n",
      "  Batch [70/107] Loss: 0.7916\n",
      "  Batch [80/107] Loss: 0.8336\n",
      "  Batch [90/107] Loss: 0.7621\n",
      "  Batch [100/107] Loss: 0.8282\n",
      "Train Loss: 0.8356\n",
      "Val mAP@0.5: 0.3320\n",
      "Learning Rate: 0.002000\n",
      "✓ Best model saved (mAP: 0.3320)\n",
      "\n",
      "======================================================================\n",
      "Epoch 6/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.7148\n",
      "  Batch [20/107] Loss: 0.8990\n",
      "  Batch [30/107] Loss: 0.6728\n",
      "  Batch [40/107] Loss: 0.7799\n",
      "  Batch [50/107] Loss: 0.7013\n",
      "  Batch [60/107] Loss: 0.5930\n",
      "  Batch [70/107] Loss: 0.6870\n",
      "  Batch [80/107] Loss: 0.6251\n",
      "  Batch [90/107] Loss: 0.6468\n",
      "  Batch [100/107] Loss: 0.9184\n",
      "Train Loss: 0.7720\n",
      "Val mAP@0.5: 0.4896\n",
      "Learning Rate: 0.002000\n",
      "✓ Best model saved (mAP: 0.4896)\n",
      "\n",
      "======================================================================\n",
      "Epoch 7/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.6150\n",
      "  Batch [20/107] Loss: 0.8018\n",
      "  Batch [30/107] Loss: 0.6803\n",
      "  Batch [40/107] Loss: 0.7804\n",
      "  Batch [50/107] Loss: 0.7193\n",
      "  Batch [60/107] Loss: 0.7180\n",
      "  Batch [70/107] Loss: 0.6698\n",
      "  Batch [80/107] Loss: 0.7689\n",
      "  Batch [90/107] Loss: 0.7538\n",
      "  Batch [100/107] Loss: 0.6681\n",
      "Train Loss: 0.7322\n",
      "Val mAP@0.5: 0.5276\n",
      "Learning Rate: 0.002000\n",
      "✓ Best model saved (mAP: 0.5276)\n",
      "\n",
      "======================================================================\n",
      "Epoch 8/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.7249\n",
      "  Batch [20/107] Loss: 0.7669\n",
      "  Batch [30/107] Loss: 0.6795\n",
      "  Batch [40/107] Loss: 0.6016\n",
      "  Batch [50/107] Loss: 0.7069\n",
      "  Batch [60/107] Loss: 0.6854\n",
      "  Batch [70/107] Loss: 0.6831\n",
      "  Batch [80/107] Loss: 0.7276\n",
      "  Batch [90/107] Loss: 0.7762\n",
      "  Batch [100/107] Loss: 0.7844\n",
      "Train Loss: 0.7234\n",
      "Val mAP@0.5: 0.5338\n",
      "Learning Rate: 0.002000\n",
      "✓ Best model saved (mAP: 0.5338)\n",
      "Checkpoint saved: models/retinanet_epoch_8.pth\n",
      "Training curves saved: visualizations/training_curves_epoch8.png\n",
      "\n",
      "======================================================================\n",
      "Epoch 9/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.8279\n",
      "  Batch [20/107] Loss: 0.7364\n",
      "  Batch [30/107] Loss: 0.6801\n",
      "  Batch [40/107] Loss: 0.8318\n",
      "  Batch [50/107] Loss: 0.7102\n",
      "  Batch [60/107] Loss: 0.6505\n",
      "  Batch [70/107] Loss: 0.6639\n",
      "  Batch [80/107] Loss: 0.6996\n",
      "  Batch [90/107] Loss: 0.5790\n",
      "  Batch [100/107] Loss: 0.6142\n",
      "Train Loss: 0.6914\n",
      "Val mAP@0.5: 0.5625\n",
      "Learning Rate: 0.001000\n",
      "✓ Best model saved (mAP: 0.5625)\n",
      "\n",
      "======================================================================\n",
      "Epoch 10/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.8124\n",
      "  Batch [20/107] Loss: 0.6946\n",
      "  Batch [30/107] Loss: 0.6423\n",
      "  Batch [40/107] Loss: 0.6034\n",
      "  Batch [50/107] Loss: 0.6538\n",
      "  Batch [60/107] Loss: 0.6373\n",
      "  Batch [70/107] Loss: 0.6745\n",
      "  Batch [80/107] Loss: 0.6646\n",
      "  Batch [90/107] Loss: 0.5809\n",
      "  Batch [100/107] Loss: 0.5734\n",
      "Train Loss: 0.6734\n",
      "Val mAP@0.5: 0.5780\n",
      "Learning Rate: 0.001000\n",
      "✓ Best model saved (mAP: 0.5780)\n",
      "\n",
      "======================================================================\n",
      "Epoch 11/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.7495\n",
      "  Batch [20/107] Loss: 0.6855\n",
      "  Batch [30/107] Loss: 0.5690\n",
      "  Batch [40/107] Loss: 0.6851\n",
      "  Batch [50/107] Loss: 0.6071\n",
      "  Batch [60/107] Loss: 0.6084\n",
      "  Batch [70/107] Loss: 0.6143\n",
      "  Batch [80/107] Loss: 0.7743\n",
      "  Batch [90/107] Loss: 0.5293\n",
      "  Batch [100/107] Loss: 1.0663\n",
      "Train Loss: 0.6646\n",
      "Val mAP@0.5: 0.5772\n",
      "Learning Rate: 0.001000\n",
      "\n",
      "======================================================================\n",
      "Epoch 12/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.7564\n",
      "  Batch [20/107] Loss: 0.7143\n",
      "  Batch [30/107] Loss: 0.5691\n",
      "  Batch [40/107] Loss: 0.7447\n",
      "  Batch [50/107] Loss: 0.7289\n",
      "  Batch [60/107] Loss: 0.5559\n",
      "  Batch [70/107] Loss: 0.6036\n",
      "  Batch [80/107] Loss: 0.6804\n",
      "  Batch [90/107] Loss: 0.7150\n",
      "  Batch [100/107] Loss: 0.6367\n",
      "Train Loss: 0.6640\n",
      "Val mAP@0.5: 0.5914\n",
      "Learning Rate: 0.001000\n",
      "✓ Best model saved (mAP: 0.5914)\n",
      "Training curves saved: visualizations/training_curves_epoch12.png\n",
      "\n",
      "======================================================================\n",
      "Epoch 13/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.6293\n",
      "  Batch [20/107] Loss: 0.8415\n",
      "  Batch [30/107] Loss: 0.5295\n",
      "  Batch [40/107] Loss: 0.6839\n",
      "  Batch [50/107] Loss: 0.6470\n",
      "  Batch [60/107] Loss: 0.5843\n",
      "  Batch [70/107] Loss: 0.7206\n",
      "  Batch [80/107] Loss: 0.6371\n",
      "  Batch [90/107] Loss: 0.6735\n",
      "  Batch [100/107] Loss: 0.5588\n",
      "Train Loss: 0.6401\n",
      "Val mAP@0.5: 0.6220\n",
      "Learning Rate: 0.001000\n",
      "✓ Best model saved (mAP: 0.6220)\n",
      "\n",
      "======================================================================\n",
      "Epoch 14/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.5450\n",
      "  Batch [20/107] Loss: 0.7683\n",
      "  Batch [30/107] Loss: 0.5894\n",
      "  Batch [40/107] Loss: 0.6977\n",
      "  Batch [50/107] Loss: 0.5678\n",
      "  Batch [60/107] Loss: 0.6473\n",
      "  Batch [70/107] Loss: 0.5853\n",
      "  Batch [80/107] Loss: 0.6013\n",
      "  Batch [90/107] Loss: 0.5535\n",
      "  Batch [100/107] Loss: 0.6236\n",
      "Train Loss: 0.6410\n",
      "Val mAP@0.5: 0.6116\n",
      "Learning Rate: 0.001000\n",
      "\n",
      "======================================================================\n",
      "Epoch 15/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.5695\n",
      "  Batch [20/107] Loss: 0.6903\n",
      "  Batch [30/107] Loss: 0.5056\n",
      "  Batch [40/107] Loss: 0.6250\n",
      "  Batch [50/107] Loss: 0.6203\n",
      "  Batch [60/107] Loss: 0.6189\n",
      "  Batch [70/107] Loss: 0.5782\n",
      "  Batch [80/107] Loss: 0.7281\n",
      "  Batch [90/107] Loss: 0.5078\n",
      "  Batch [100/107] Loss: 0.6383\n",
      "Train Loss: 0.6371\n",
      "Val mAP@0.5: 0.6393\n",
      "Learning Rate: 0.001000\n",
      "✓ Best model saved (mAP: 0.6393)\n",
      "\n",
      "======================================================================\n",
      "Epoch 16/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.5646\n",
      "  Batch [20/107] Loss: 0.6601\n",
      "  Batch [30/107] Loss: 0.5484\n",
      "  Batch [40/107] Loss: 0.5522\n",
      "  Batch [50/107] Loss: 0.7983\n",
      "  Batch [60/107] Loss: 0.6309\n",
      "  Batch [70/107] Loss: 0.5426\n",
      "  Batch [80/107] Loss: 0.7345\n",
      "  Batch [90/107] Loss: 0.5150\n",
      "  Batch [100/107] Loss: 0.5595\n",
      "Train Loss: 0.6195\n",
      "Val mAP@0.5: 0.6184\n",
      "Learning Rate: 0.001000\n",
      "Checkpoint saved: models/retinanet_epoch_16.pth\n",
      "Training curves saved: visualizations/training_curves_epoch16.png\n",
      "\n",
      "======================================================================\n",
      "Epoch 17/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.5684\n",
      "  Batch [20/107] Loss: 0.7606\n",
      "  Batch [30/107] Loss: 0.6214\n",
      "  Batch [40/107] Loss: 0.5692\n",
      "  Batch [50/107] Loss: 0.6056\n",
      "  Batch [60/107] Loss: 0.4544\n",
      "  Batch [70/107] Loss: 0.4995\n",
      "  Batch [80/107] Loss: 0.5577\n",
      "  Batch [90/107] Loss: 0.6688\n",
      "  Batch [100/107] Loss: 0.6124\n",
      "Train Loss: 0.6104\n",
      "Val mAP@0.5: 0.6289\n",
      "Learning Rate: 0.000500\n",
      "\n",
      "======================================================================\n",
      "Epoch 18/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.5271\n",
      "  Batch [20/107] Loss: 0.6437\n",
      "  Batch [30/107] Loss: 0.5333\n",
      "  Batch [40/107] Loss: 0.5241\n",
      "  Batch [50/107] Loss: 0.5350\n",
      "  Batch [60/107] Loss: 0.4928\n",
      "  Batch [70/107] Loss: 0.6176\n",
      "  Batch [80/107] Loss: 0.5403\n",
      "  Batch [90/107] Loss: 0.6206\n",
      "  Batch [100/107] Loss: 0.6114\n",
      "Train Loss: 0.6039\n",
      "Val mAP@0.5: 0.6400\n",
      "Learning Rate: 0.000500\n",
      "✓ Best model saved (mAP: 0.6400)\n",
      "\n",
      "======================================================================\n",
      "Epoch 19/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.5442\n",
      "  Batch [20/107] Loss: 0.6544\n",
      "  Batch [30/107] Loss: 0.4799\n",
      "  Batch [40/107] Loss: 0.6508\n",
      "  Batch [50/107] Loss: 0.5084\n",
      "  Batch [60/107] Loss: 0.5245\n",
      "  Batch [70/107] Loss: 0.5899\n",
      "  Batch [80/107] Loss: 0.6729\n",
      "  Batch [90/107] Loss: 0.4788\n",
      "  Batch [100/107] Loss: 0.6643\n",
      "Train Loss: 0.5992\n",
      "Val mAP@0.5: 0.6334\n",
      "Learning Rate: 0.000500\n",
      "\n",
      "======================================================================\n",
      "Epoch 20/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.5519\n",
      "  Batch [20/107] Loss: 0.6637\n",
      "  Batch [30/107] Loss: 0.5009\n",
      "  Batch [40/107] Loss: 0.6269\n",
      "  Batch [50/107] Loss: 0.6592\n",
      "  Batch [60/107] Loss: 0.5829\n",
      "  Batch [70/107] Loss: 0.5023\n",
      "  Batch [80/107] Loss: 0.5697\n",
      "  Batch [90/107] Loss: 0.6775\n",
      "  Batch [100/107] Loss: 0.5644\n",
      "Train Loss: 0.5976\n",
      "Val mAP@0.5: 0.6422\n",
      "Learning Rate: 0.000500\n",
      "✓ Best model saved (mAP: 0.6422)\n",
      "Training curves saved: visualizations/training_curves_epoch20.png\n",
      "\n",
      "======================================================================\n",
      "Epoch 21/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.5803\n",
      "  Batch [20/107] Loss: 0.7616\n",
      "  Batch [30/107] Loss: 0.5285\n",
      "  Batch [40/107] Loss: 0.6300\n",
      "  Batch [50/107] Loss: 0.7695\n",
      "  Batch [60/107] Loss: 0.5520\n",
      "  Batch [70/107] Loss: 0.5811\n",
      "  Batch [80/107] Loss: 0.6101\n",
      "  Batch [90/107] Loss: 0.5070\n",
      "  Batch [100/107] Loss: 0.4912\n",
      "Train Loss: 0.5985\n",
      "Val mAP@0.5: 0.6485\n",
      "Learning Rate: 0.000500\n",
      "✓ Best model saved (mAP: 0.6485)\n",
      "\n",
      "======================================================================\n",
      "Epoch 22/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.5687\n",
      "  Batch [20/107] Loss: 0.6488\n",
      "  Batch [30/107] Loss: 0.4896\n",
      "  Batch [40/107] Loss: 0.7453\n",
      "  Batch [50/107] Loss: 0.6124\n",
      "  Batch [60/107] Loss: 0.4850\n",
      "  Batch [70/107] Loss: 0.5281\n",
      "  Batch [80/107] Loss: 0.5821\n",
      "  Batch [90/107] Loss: 0.5249\n",
      "  Batch [100/107] Loss: 0.5926\n",
      "Train Loss: 0.5918\n",
      "Val mAP@0.5: 0.6501\n",
      "Learning Rate: 0.000500\n",
      "✓ Best model saved (mAP: 0.6501)\n",
      "\n",
      "======================================================================\n",
      "Epoch 23/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.5553\n",
      "  Batch [20/107] Loss: 0.7126\n",
      "  Batch [30/107] Loss: 0.5794\n",
      "  Batch [40/107] Loss: 0.6183\n",
      "  Batch [50/107] Loss: 0.6692\n",
      "  Batch [60/107] Loss: 0.6253\n",
      "  Batch [70/107] Loss: 0.5404\n",
      "  Batch [80/107] Loss: 0.6467\n",
      "  Batch [90/107] Loss: 0.6211\n",
      "  Batch [100/107] Loss: 0.6239\n",
      "Train Loss: 0.5944\n",
      "Val mAP@0.5: 0.6537\n",
      "Learning Rate: 0.000500\n",
      "✓ Best model saved (mAP: 0.6537)\n",
      "\n",
      "======================================================================\n",
      "Epoch 24/24\n",
      "======================================================================\n",
      "  Batch [10/107] Loss: 0.5695\n",
      "  Batch [20/107] Loss: 0.7339\n",
      "  Batch [30/107] Loss: 0.4732\n",
      "  Batch [40/107] Loss: 0.5597\n",
      "  Batch [50/107] Loss: 0.5334\n",
      "  Batch [60/107] Loss: 0.5981\n",
      "  Batch [70/107] Loss: 0.5383\n",
      "  Batch [80/107] Loss: 0.6150\n",
      "  Batch [90/107] Loss: 0.5564\n",
      "  Batch [100/107] Loss: 0.5229\n",
      "Train Loss: 0.5973\n",
      "Val mAP@0.5: 0.6397\n",
      "Learning Rate: 0.000500\n",
      "Checkpoint saved: models/retinanet_epoch_24.pth\n",
      "Training curves saved: visualizations/training_curves_epoch24.png\n",
      "\n",
      "======================================================================\n",
      "Training completed.\n",
      "Best validation mAP@0.5: 0.6537\n",
      "======================================================================\n",
      "Training curves saved: visualizations/training_curves_final.png\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type Tensor is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Save final visualizations and metrics\u001b[39;00m\n\u001b[32m     85\u001b[39m plot_training_curves(history, \u001b[33m'\u001b[39m\u001b[33mvisualizations/training_curves_final.png\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[43msave_training_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels/training_metrics.json\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Save final model\u001b[39;00m\n\u001b[32m     89\u001b[39m torch.save(model.state_dict(), \u001b[33m'\u001b[39m\u001b[33mmodels/retinanet_final.pth\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36msave_training_metrics\u001b[39m\u001b[34m(history, save_path)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Export metrics for later analysis or reporting.\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMetrics saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/json/__init__.py:179\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[39m\n\u001b[32m    173\u001b[39m     iterable = \u001b[38;5;28mcls\u001b[39m(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n\u001b[32m    174\u001b[39m         check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n\u001b[32m    175\u001b[39m         separators=separators,\n\u001b[32m    176\u001b[39m         default=default, sort_keys=sort_keys, **kw).iterencode(obj)\n\u001b[32m    177\u001b[39m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/json/encoder.py:442\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    440\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    444\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/json/encoder.py:411\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_dict\u001b[39m\u001b[34m(dct, _current_indent_level)\u001b[39m\n\u001b[32m    409\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    410\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/json/encoder.py:324\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode_list\u001b[39m\u001b[34m(lst, _current_indent_level)\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    323\u001b[39m             chunks = _iterencode(value, _current_indent_level)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/json/encoder.py:449\u001b[39m, in \u001b[36m_make_iterencode.<locals>._iterencode\u001b[39m\u001b[34m(o, _current_indent_level)\u001b[39m\n\u001b[32m    447\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCircular reference detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    448\u001b[39m     markers[markerid] = o\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m newobj = \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    451\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(newobj, _current_indent_level)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.14/3.14.0_1/Frameworks/Python.framework/Versions/3.14/lib/python3.14/json/encoder.py:180\u001b[39m, in \u001b[36mJSONEncoder.default\u001b[39m\u001b[34m(self, o)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    178\u001b[39m \n\u001b[32m    179\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    181\u001b[39m                     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mis not JSON serializable\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Object of type Tensor is not JSON serializable",
      "when serializing list item 0",
      "when serializing dict item 'val_map50'"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr = config.LEARNING_RATE,\n",
    "    momentum = config.MOMENTUM,\n",
    "    weight_decay = config.WEIGHT_DECAY # >>> double check this value <<<\n",
    ")\n",
    "\n",
    "# Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size = config.STEP_SIZE, \n",
    "    gamma = config.GAMMA\n",
    ")\n",
    "\n",
    "# Checkpoint directory\n",
    "Path(\"models\").mkdir(exist_ok = True)\n",
    "Path(\"visualizations\").mkdir(exist_ok = True)\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_map = 0\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Starting training: {config.NUM_EPOCHS} epochs\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE} | LR: {config.LEARNING_RATE}\")\n",
    "print(f\"Device: {device} | Workers: {config.NUM_WORKERS}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    print(f\"\\n{\"=\" * 70}\")\n",
    "    print(f\"Epoch {epoch + 1}/{config.NUM_EPOCHS}\")\n",
    "    print(f\"{\"=\" * 70}\")\n",
    "    \n",
    "    # Training\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    \n",
    "    # Validation\n",
    "    results = evaluate(model, val_loader, device)\n",
    "    val_map50 = results[\"map_50\"]\n",
    "\n",
    "    # Update learning rate\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    scheduler.step()\n",
    "\n",
    "    # Store metrics\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_map50\"].append(val_map50)\n",
    "    history[\"learning_rate\"].append(current_lr)\n",
    "    \n",
    "    # Display progress\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val mAP@0.5: {val_map50:.4f}\")\n",
    "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_map50 > best_map:\n",
    "        best_map = val_map50\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_map': best_map,\n",
    "        }, 'models/retinanet_best.pth')\n",
    "        print(f\"✓ Best model saved (mAP: {best_map:.4f})\")\n",
    "    \n",
    "    # Periodic checkpoint\n",
    "    if (epoch + 1) % 8 == 0:\n",
    "        checkpoint_path = f\"models/retinanet_epoch_{epoch+1}.pth\"\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    # Generate visualizations\n",
    "    if config.SAVE_PLOTS and (epoch + 1) % config.PLOT_INTERVAL == 0:\n",
    "        plot_path = f\"visualizations/training_curves_epoch{epoch+1}.png\"\n",
    "        plot_training_curves(history, plot_path)\n",
    "\n",
    "# Final outputs\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Training completed.\")\n",
    "print(f\"Best validation mAP@0.5: {best_map:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save final visualizations and metrics\n",
    "plot_training_curves(history, 'visualizations/training_curves_final.png')\n",
    "save_training_metrics(history, 'models/training_metrics.json')\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), 'models/retinanet_final.pth')\n",
    "print(\"Final model saved: models/retinanet_final.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad18874",
   "metadata": {},
   "source": [
    "# 9/ Training Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7f0af80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Summary\n",
      "======================================================================\n",
      "Total epochs: 24\n",
      "Final train loss: 0.5973\n",
      "Final val mAP@0.5: 0.6397\n",
      "Best val mAP@0.5: 0.6537\n",
      "\n",
      "Improvement: 0.6397\n",
      "\n",
      "Generated Files:\n",
      "- visualizations/training_curves_final.png\n",
      "- models/training_metrics.json\n",
      "- models/retinanet_best.pth\n",
      "- models/retinanet_final.pth\n"
     ]
    }
   ],
   "source": [
    "# Display final metrics\n",
    "print(\"\\nTraining Summary\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total epochs: {len(history['train_loss'])}\")\n",
    "print(f\"Final train loss: {history['train_loss'][-1]:.4f}\")\n",
    "print(f\"Final val mAP@0.5: {history['val_map50'][-1]:.4f}\")\n",
    "print(f\"Best val mAP@0.5: {max(history['val_map50']):.4f}\")\n",
    "print(f\"\\nImprovement: {history['val_map50'][-1] - history['val_map50'][0]:.4f}\")\n",
    "\n",
    "# Show visualization paths\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(\"- visualizations/training_curves_final.png\")\n",
    "print(\"- models/training_metrics.json\")\n",
    "print(\"- models/retinanet_best.pth\")\n",
    "print(\"- models/retinanet_final.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-CVsteel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
