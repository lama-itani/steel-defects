{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7b55a8",
   "metadata": {},
   "source": [
    "# 1/ Env Setup\n",
    "Load necessary libraries to run this notebook. <br>\n",
    "All libraries are cited in ```requirements.txt```. <br>\n",
    "Documentation: https://docs.pytorch.org/vision/main/models/generated/torchvision.models.detection.retinanet_resnet50_fpn_v2.html\n",
    "\n",
    "## 1.1/ Import dependencies\n",
    "Load libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99a2cafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added to sys.path: /Users/litani/Documents/myCode/steel-defects\n"
     ]
    }
   ],
   "source": [
    "import sys  \n",
    "import os\n",
    "\n",
    "current_dir = os.getcwd() # path to the current working directory (notebook location)\n",
    "project_root = os.path.abspath(os.path.join(current_dir, '..')) # path to project root\n",
    "\n",
    "if project_root not in sys.path: # add project root to sys.path\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"Project root added to sys.path: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce85e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch \n",
    "import torchvision\n",
    "from torchvision.models.detection import retinanet_resnet50_fpn_v2\n",
    "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\n",
    "import albumentations as A # note that albumentationsx was installed and but you still albumentations\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "from src.utils.parse_xml import parse_xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c148bd",
   "metadata": {},
   "source": [
    "## 1.2/ Set reproducibility\n",
    "Device and seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf6e9a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba1f95b",
   "metadata": {},
   "source": [
    "# 2/ Configuration Management\n",
    "Define:\n",
    "- image path\n",
    "- model hyperparameters\n",
    "- hardware\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eda84995",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths\n",
    "    DATA_ROOT = Path(\"../data/raw\")\n",
    "    TRAIN_IMG = DATA_ROOT / \"train_images\"\n",
    "    TRAIN_ANN = DATA_ROOT / \"train_annotations\"\n",
    "    VAL_IMG = DATA_ROOT / \"valid_images\"\n",
    "    VAL_ANN = DATA_ROOT / \"valid_annotations\"\n",
    "\n",
    "    # Model parameters\n",
    "    NUM_CLASSES = 7 # 6 defects + 1 background\n",
    "    BACKBONE_PRETRAINED = True \n",
    "    \n",
    "    # Training hyperparameters\n",
    "    BATCH_SIZE = 16  # no mention of batch size in the paper\n",
    "    NUM_EPOCHS = 5 # 24 epochs based on paper. Reduced for quicker testing\n",
    "    LEARNING_RATE = 0.0025 # 0.0025 based on paper\n",
    "    MOMENTUM = 0.9 # 0.9 based on paper\n",
    "\n",
    "    # Hardware\n",
    "    DEVICE = device\n",
    "    NUM_WORKERS = 8\n",
    "    PIN_MEMORY = True if torch.cuda.is_available() else False\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2d0cc310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Config at 0x144f9fcb0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33c203",
   "metadata": {},
   "source": [
    "# 3/ Dataset Class\n",
    "- Load images and annotations into PyTorch format. \n",
    "- This is necessary since RetineNet excepts a dictionary format. The latter requires XML parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2eb61e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteelDefectDataset(torch.utils.data.Dataset): # inherits from PyTorch Dataaset class, making it compatible w/ DataLoader for batch and // processing\n",
    "    def __init__(self, img_dir, ann_dir, transforms = None): # takes 3 inputs: image directory, annotation directory, and optional transforms (default no augmentation)\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.ann_dir = Path(ann_dir)\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Logic of loading a dataset\n",
    "        self.images = sorted(list(self.img_dir.glob(\"*.jpg\"))) # find all .jpg files, sort them in order\n",
    "        self.class_map = { \n",
    "            \"crazing\" : 1,\n",
    "            \"inclusion\" : 2,\n",
    "            \"patches\" : 3,\n",
    "            \"pitted_surface\" : 4,\n",
    "            \"rolled-in_scale\" : 5,\n",
    "            \"scratches\" : 6,\n",
    "        }                                                  # mapping defect names (catagorical) to integers (numerical)\n",
    "    def __len__(self): # runs total number of images\n",
    "        return len(self.images) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.images[idx]\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # convert BGR, i.e. OpenCV format, to RGB, i.e. PyTorch/RetinaNet format\n",
    "\n",
    "        # Parse annotations using parse_xml function\n",
    "        xml_path = self.ann_dir / img_path.name.replace(\".jpg\", \".xml\")\n",
    "        boxes_data = parse_xml(str(xml_path))\n",
    "\n",
    "        # Convert XML to lists: [x1, y1, x2, y2], labels = integers\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for box in boxes_data:\n",
    "            boxes.append([box[\"xmin\"], box[\"ymin\"], box[\"xmax\"], box[\"ymax\"]])\n",
    "            labels.append(self.class_map[box[\"label\"]])\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.as_tensor(boxes, dtype = torch.float32),\n",
    "            \"labels\": torch.as_tensor(labels, dtype = torch.int64),\n",
    "            \"image_id\": torch.as_tensor([idx])\n",
    "        } # lists are packages into dictionary to be feed to RetinaNet, becoming PyTorch tensors.\n",
    "\n",
    "        # Image augmetations: returns a tuple (image tensor, target dictionary)\n",
    "        if self.transforms:\n",
    "            transformed = self.transforms(image = image, bboxes = target[\"boxes\"], labels = target[\"labels\"])\n",
    "            image = transformed[\"image\"]\n",
    "            target[\"boxes\"] = torch.as_tensor(transformed[\"bboxes\"], dtype = torch.float32)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038647b",
   "metadata": {},
   "source": [
    "# 4/ Data Augmentation\n",
    "- We have 1800 images, resorting to image augmentation is mandatory to avoid overfitting. \n",
    "- Geometric transformation, simplist form, will be applied as a quick fix:\n",
    "    - Horizental/Vertical flips\n",
    "    - Rotate by 90\n",
    "    - Others: brightness, contrast, adding random noise\n",
    "- **NB:** OpenCV stores images as [Height in pixels, Width in pixels, RGB] while PyTorch expects [channel, height, width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d62e0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p = .5), # probability of 50% to flip the image left-right >> defects visible from any orientation\n",
    "        A.VerticalFlip(p = .5), # flip top-bottom >> surfaces viewed from any angle\n",
    "        A.RandomRotate90(p = .5), # rotate 90, 180 or 270 degrees >> increase orientation variety\n",
    "        A.RandomBrightnessContrast(p = .3), # adjust brightness/contrast >> image quality\n",
    "        A.GaussNoise(p = .2), # add random noise\n",
    "        ToTensorV2() # converts NumPy array to PyTorch tensor, divides pix by 255 ([0,1] range), permutes format from [H,W,C] to [C,H,W].\n",
    "    ], bbox_params = A.BboxParams(format = \"pascal_voc\", label_fields = [\"labels\"])) # XML annotation has a Pascal Voc format\n",
    "\n",
    "def get_val_transforms(): # for validation on clean images, converting only to tensor format, no augmentation.\n",
    "    return A.Compose([\n",
    "        ToTensorV2()\n",
    "    ], bbox_params = A.BboxParams(format = \"pascal_voc\", label_fields = [\"labels\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2aec8b1",
   "metadata": {},
   "source": [
    "# 5/ Model Initilization\n",
    "- Apply transfer learning where pretrained RetineNet is loaded the changes are applied based on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60474ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes, pretrained = True):\n",
    "    # Load pretrained RetineNet w/ ResNet50 backbone,\n",
    "    model = retinanet_resnet50_fpn_v2(wights = \"DEFAULT\" if pretrained else None)  # DEFAULT loads ImageNet pretrained weights for transfer learning\n",
    "    \n",
    "    # Replace head so that model learns defect-specific patterns\n",
    "    num_anchors = model.head.classification_head.num_anchors # default is 9 anchors per location >> 3 scales x 3 aspect ratios\n",
    "    model.head.classification_head = RetinaNetClassificationHead(\n",
    "        in_channels = 256,          # Input: 256 features from FPN   \n",
    "        num_anchors = num_anchors,  # Process: 9 anchors per location\n",
    "        num_classes = num_classes   # Output: 7 classes scores per anchor\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model(config.NUM_CLASSES).to(device) # Create model instance and move to device CPU/GPU, config.NUM_CLASSES = 7 includes background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e974a65a",
   "metadata": {},
   "source": [
    "# 6/ Data Loaders\n",
    "- collate_func is a fucntion that works on the collation process of RetinaNet since images have variable bbox counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b547d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset with augmentations and val dataset without augmentations, only format conversion\n",
    "train_dataset = SteelDefectDataset(\n",
    "    config.TRAIN_IMG,\n",
    "    config.TRAIN_ANN,\n",
    "    transforms = get_train_transforms()\n",
    ")\n",
    "\n",
    "val_dataset = SteelDefectDataset(\n",
    "    config.VAL_IMG,\n",
    "    config.VAL_ANN,\n",
    "    transforms = get_val_transforms()\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "def collate_func(batch):\n",
    "    return tuple(zip(*batch)) # transpose batch to a list of tensors instead of stack of images into a single tensor \n",
    "                                #[(img1, target1), (img2, target2)] >> ([img1, img2],[target1, target2])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = config.BATCH_SIZE,\n",
    "    shuffle = True, # shuffle training data for better generalization\n",
    "    num_workers = config.NUM_WORKERS,\n",
    "    pin_memory = config.PIN_MEMORY, # only useful if using GPU\n",
    "    collate_fn = collate_func \n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = config.BATCH_SIZE,\n",
    "    shuffle = False, # validation data should be consistent\n",
    "    num_workers = config.NUM_WORKERS,\n",
    "    pin_memory = config.PIN_MEMORY,\n",
    "    collate_fn = collate_func\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-CVsteel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
